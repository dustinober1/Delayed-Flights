{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Delay Prediction - Model Training and Evaluation\n",
    "\n",
    "This notebook focuses on training and evaluating multiple machine learning models for flight delay prediction.\n",
    "\n",
    "## Objectives\n",
    "1. Load engineered features from previous notebook\n",
    "2. Prepare data for machine learning (train/test split, handling imbalance)\n",
    "3. Train multiple models (Logistic Regression, Random Forest, XGBoost, LightGBM)\n",
    "4. Evaluate and compare model performance\n",
    "5. Perform hyperparameter tuning\n",
    "6. Analyze feature importance\n",
    "7. Save best model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from models.train_model import FlightDelayPredictor, train_all_models\n",
    "from visualization.plots import FlightDelayVisualizer\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered dataset\n",
    "data_path = '../data/processed/flight_features_engineered.csv'\n",
    "metadata_path = '../data/processed/feature_metadata.json'\n",
    "\n",
    "if Path(data_path).exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded engineered dataset: {df.shape}\")\n",
    "    \n",
    "    # Load metadata if available\n",
    "    if Path(metadata_path).exists():\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"Feature categories: {metadata['feature_categories']}\")\n",
    "        print(f\"Delay rate: {metadata['delay_rate']*100:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Engineered data not found at {data_path}\")\n",
    "    print(\"Please run 02_feature_engineering.ipynb first\")\n",
    "    \n",
    "    # Fallback: load and quickly engineer features\n",
    "    from data.download_data import load_airline_data\n",
    "    from features.feature_engineering import FlightFeatureEngineer\n",
    "    \n",
    "    print(\"Loading and engineering features as fallback...\")\n",
    "    df_raw = load_airline_data(year=2023, sample_size=50000)\n",
    "    \n",
    "    if df_raw is not None:\n",
    "        df_raw['delayed'] = (df_raw['ARR_DELAY'] > 15).astype(int)\n",
    "        df_raw = df_raw[(df_raw.get('CANCELLED', 0) != 1) & (df_raw['ARR_DELAY'].notna())].copy()\n",
    "        \n",
    "        engineer = FlightFeatureEngineer()\n",
    "        df = engineer.engineer_all_features(df_raw)\n",
    "        print(f\"Engineered features: {df.shape}\")\n",
    "    else:\n",
    "        raise ValueError(\"Could not load data\")\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Target distribution: {df['delayed'].value_counts()}\")\n",
    "print(f\"Delay rate: {df['delayed'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize predictor\n",
    "predictor = FlightDelayPredictor()\n",
    "viz = FlightDelayVisualizer()\n",
    "\n",
    "# Prepare data for modeling\n",
    "print(\"Preparing data for modeling...\")\n",
    "X_train, X_test, y_train, y_test = predictor.prepare_data(df, target_col='delayed', test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Feature columns: {len(predictor.feature_names)}\")\n",
    "\n",
    "# Show class distribution\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(f\"On-time: {(y_train == 0).sum():,} ({(y_train == 0).mean()*100:.1f}%)\")\n",
    "print(f\"Delayed: {(y_train == 1).sum():,} ({(y_train == 1).mean()*100:.1f}%)\")\n",
    "print(f\"Class imbalance ratio: {(y_train == 0).sum() / (y_train == 1).sum():.1f}:1\")\n",
    "\n",
    "# Display sample of features\n",
    "print(f\"\\nSample features (first 10):\")\n",
    "print(predictor.feature_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline logistic regression\n",
    "print(\"Training baseline logistic regression model...\")\n",
    "lr_model = predictor.train_logistic_regression(X_train, y_train)\n",
    "\n",
    "# Evaluate baseline\n",
    "lr_result = predictor.evaluate_model(lr_model, X_test, y_test, \"Logistic Regression (Baseline)\")\n",
    "\n",
    "# Plot precision-recall curve for baseline\n",
    "predictor.plot_precision_recall_curve(y_test, lr_result['probabilities'], \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation set for tree-based models\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set (for tree models): {X_train_split.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = predictor.train_random_forest(X_train, y_train)\n",
    "rf_result = predictor.evaluate_model(rf_model, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_model = predictor.train_xgboost(X_train_split, y_train_split, X_val, y_val)\n",
    "xgb_result = predictor.evaluate_model(xgb_model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# Train LightGBM\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "lgb_model = predictor.train_lightgbm(X_train_split, y_train_split, X_val, y_val)\n",
    "lgb_result = predictor.evaluate_model(lgb_model, X_test, y_test, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results = predictor.compare_models(X_test, y_test)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': [r['model_name'] for r in results],\n",
    "    'ROC_AUC': [r['roc_auc'] for r in results]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance Ranking:\")\n",
    "display(comparison_df.sort_values('ROC_AUC', ascending=False))\n",
    "\n",
    "# Visualize model comparison\n",
    "viz.plot_model_comparison(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for result in results:\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "    plt.plot(fpr, tpr, label=f\"{result['model_name']} (AUC = {result['roc_auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Model Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for result in results:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, result['probabilities'])\n",
    "    avg_precision = average_precision_score(y_test, result['probabilities'])\n",
    "    plt.plot(recall, precision, label=f\"{result['model_name']} (AP = {avg_precision:.3f})\")\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - Model Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for best performing tree-based model\n",
    "best_model_name = max(results, key=lambda x: x['roc_auc'])['model_name']\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "best_model = predictor.best_model\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\nAnalyzing feature importance...\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = predictor.plot_feature_importance(best_model, best_model_name, top_n=20)\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    importance_df = pd.DataFrame(feature_importance, columns=['Feature', 'Importance'])\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    display(importance_df.head(15))\n",
    "    \n",
    "    # Categorize important features\n",
    "    feature_categories = {\n",
    "        'Time': ['hour', 'day', 'month', 'weekend', 'sin', 'cos'],\n",
    "        'Airport/Route': ['origin', 'dest', 'route', 'major', 'departures', 'arrivals'],\n",
    "        'Aircraft': ['prev_flight', 'aircraft', 'tail', 'hours_since'],\n",
    "        'Airline': ['airline', 'carrier'],\n",
    "        'Weather': ['weather', 'temperature', 'wind', 'visibility', 'precipitation']\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFeature Importance by Category:\")\n",
    "    for category, keywords in feature_categories.items():\n",
    "        category_features = importance_df[importance_df['Feature'].str.contains('|'.join(keywords), case=False)]\n",
    "        if len(category_features) > 0:\n",
    "            total_importance = category_features['Importance'].sum()\n",
    "            print(f\"{category}: {total_importance:.3f} ({len(category_features)} features)\")\n",
    "\n",
    "else:\n",
    "    print(f\"Feature importance not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for best model type\n",
    "print(\"Performing hyperparameter tuning...\")\n",
    "print(\"Note: This may take several minutes\")\n",
    "\n",
    "# Determine which model to tune based on best performance\n",
    "if 'xgboost' in best_model_name.lower():\n",
    "    tuned_model = predictor.hyperparameter_tuning(X_train_split, y_train_split, model_type='xgboost')\n",
    "    tuned_result = predictor.evaluate_model(tuned_model, X_test, y_test, \"XGBoost (Tuned)\")\n",
    "elif 'random forest' in best_model_name.lower():\n",
    "    tuned_model = predictor.hyperparameter_tuning(X_train_split, y_train_split, model_type='random_forest')\n",
    "    tuned_result = predictor.evaluate_model(tuned_model, X_test, y_test, \"Random Forest (Tuned)\")\n",
    "else:\n",
    "    print(f\"Hyperparameter tuning not implemented for {best_model_name}\")\n",
    "    tuned_model = best_model\n",
    "    tuned_result = max(results, key=lambda x: x['roc_auc'])\n",
    "\n",
    "print(f\"\\nTuning Results:\")\n",
    "print(f\"Original {best_model_name}: {max(results, key=lambda x: x['roc_auc'])['roc_auc']:.4f}\")\n",
    "if 'tuned_result' in locals():\n",
    "    print(f\"Tuned model: {tuned_result['roc_auc']:.4f}\")\n",
    "    improvement = tuned_result['roc_auc'] - max(results, key=lambda x: x['roc_auc'])['roc_auc']\n",
    "    print(f\"Improvement: {improvement:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretability and Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "best_predictions = max(results, key=lambda x: x['roc_auc'])['predictions']\n",
    "best_probabilities = max(results, key=lambda x: x['roc_auc'])['probabilities']\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['On-Time', 'Delayed'], \n",
    "            yticklabels=['On-Time', 'Delayed'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\nDetailed Performance Metrics for {best_model_name}:\")\n",
    "print(f\"Accuracy: {(tp + tn) / (tp + tn + fp + fn):.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.3f}\")\n",
    "print(f\"Specificity: {specificity:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"ROC-AUC: {max(results, key=lambda x: x['roc_auc'])['roc_auc']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction probability distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Probability distribution by actual class\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(best_probabilities[y_test == 0], bins=50, alpha=0.7, label='Actual On-Time', color='green')\n",
    "plt.hist(best_probabilities[y_test == 1], bins=50, alpha=0.7, label='Actual Delayed', color='red')\n",
    "plt.xlabel('Predicted Delay Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Probability Distribution')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Calibration plot\n",
    "plt.subplot(1, 2, 2)\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, best_probabilities, n_bins=10)\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=best_model_name)\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Plot')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on best model\n",
    "print(\"Performing 5-fold cross-validation...\")\n",
    "\n",
    "if 'tuned_model' in locals():\n",
    "    cv_model = tuned_model\n",
    "    cv_name = f\"{best_model_name} (Tuned)\"\n",
    "else:\n",
    "    cv_model = best_model\n",
    "    cv_name = best_model_name\n",
    "\n",
    "# Use a sample for cross-validation if dataset is large\n",
    "sample_size = min(10000, len(X_train))\n",
    "if len(X_train) > sample_size:\n",
    "    X_cv_sample = X_train.sample(sample_size, random_state=42)\n",
    "    y_cv_sample = y_train[X_cv_sample.index]\n",
    "    print(f\"Using sample of {sample_size} for cross-validation\")\n",
    "else:\n",
    "    X_cv_sample = X_train\n",
    "    y_cv_sample = y_train\n",
    "\n",
    "cv_scores = cross_val_score(cv_model, X_cv_sample, y_cv_sample, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(f\"\\nCross-Validation Results for {cv_name}:\")\n",
    "print(f\"ROC-AUC Scores: {cv_scores}\")\n",
    "print(f\"Mean ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "print(f\"Test Set ROC-AUC: {max(results, key=lambda x: x['roc_auc'])['roc_auc']:.4f}\")\n",
    "\n",
    "# Plot cross-validation scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot(cv_scores)\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.title(f'Cross-Validation Scores - {cv_name}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "final_model = tuned_model if 'tuned_model' in locals() else best_model\n",
    "final_model_name = f\"best_flight_delay_model_{best_model_name.lower().replace(' ', '_')}\"\n",
    "\n",
    "predictor.save_model(final_model, final_model_name)\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_type': best_model_name,\n",
    "    'performance_metrics': {\n",
    "        'roc_auc': max(results, key=lambda x: x['roc_auc'])['roc_auc'],\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': (tp + tn) / (tp + tn + fp + fn)\n",
    "    },\n",
    "    'cross_validation': {\n",
    "        'mean_cv_score': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'cv_scores': cv_scores.tolist()\n",
    "    },\n",
    "    'training_info': {\n",
    "        'training_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'num_features': len(predictor.feature_names),\n",
    "        'feature_names': predictor.feature_names,\n",
    "        'class_distribution': {\n",
    "            'on_time': int((y_train == 0).sum()),\n",
    "            'delayed': int((y_train == 1).sum())\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add feature importance if available\n",
    "if 'feature_importance' in locals():\n",
    "    model_metadata['feature_importance'] = {\n",
    "        feature: float(importance) for feature, importance in feature_importance[:20]\n",
    "    }\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = f'../models/{final_model_name}_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Model and metadata saved:\")\n",
    "print(f\"Model: ../models/{final_model_name}.joblib\")\n",
    "print(f\"Metadata: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FLIGHT DELAY PREDICTION - FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 DATASET SUMMARY:\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Features engineered: {len(predictor.feature_names)}\")\n",
    "print(f\"Delay rate: {y_train.mean()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model_name}\")\n",
    "print(f\"ROC-AUC Score: {max(results, key=lambda x: x['roc_auc'])['roc_auc']:.4f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"Cross-validation mean: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "if 'feature_importance' in locals():\n",
    "    print(f\"\\n🎯 TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "    for i, (feature, importance) in enumerate(feature_importance[:5], 1):\n",
    "        print(f\"{i}. {feature}: {importance:.3f}\")\n",
    "\n",
    "print(f\"\\n💡 KEY INSIGHTS:\")\n",
    "print(f\"• Model successfully predicts flight delays with {max(results, key=lambda x: x['roc_auc'])['roc_auc']*100:.1f}% AUC\")\n",
    "print(f\"• Can identify {recall*100:.1f}% of actual delays (recall)\")\n",
    "print(f\"• {precision*100:.1f}% of predicted delays are correct (precision)\")\n",
    "print(f\"• Class imbalance handled effectively with appropriate techniques\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"1. Deploy model using Streamlit app (app/streamlit_app.py)\")\n",
    "print(f\"2. Integrate real-time weather data for live predictions\")\n",
    "print(f\"3. Monitor model performance and retrain periodically\")\n",
    "print(f\"4. Consider ensemble methods for further improvement\")\n",
    "print(f\"5. Collect feedback and iterate on features\")\n",
    "\n",
    "print(f\"\\n📁 SAVED FILES:\")\n",
    "print(f\"• Model: ../models/{final_model_name}.joblib\")\n",
    "print(f\"• Metadata: ../models/{final_model_name}_metadata.json\")\n",
    "print(f\"• Processed data: ../data/processed/flight_features_engineered.csv\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 MODEL TRAINING COMPLETE!\")\n",
    "print(\"Ready for deployment and real-world testing.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}